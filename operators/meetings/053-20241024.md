# ONNX Operators SIG meeting
# Thursday, October 24th, 2024 at 9:00am PST

## Agenda

* Infrastructure SIG updates
* [PR 6461](https://github.com/onnx/onnx/pull/6461): Add Rotary Embedding op
* [PR 6443](https://github.com/onnx/onnx/pull/6443): Add LLM Normalization ops
* [Issue 6408](https://github.com/onnx/onnx/pull/6408): Spec clarification for random ops

## Attendees

* Alexandre Eichenberger (IBM)
* Ganesan Ramalingam (Microsoft)
* George Nash (Intel)
* Justin Chu (Microsoft)
* Michal Karzy≈Ñski (Intel)
* Raghavan Naresh Sarangapani (Intel)
* Shubham Bhokare (Microsoft)
* Yuan Yao (Nvidia)

## Meeting Minutes

### Infrastructure SIG updates

The 1.17 release is out!

### [PR 6461](https://github.com/onnx/onnx/pull/6461): Add Rotary Embedding op

Shubham summarized his PR adding a Rotary Embedding op to the ONNX standard. It
covers the three different variants of rotary embeddings used in practice, the base
and partial rotation and interleaved. Comments and feedback welcome.

### [PR 6443](https://github.com/onnx/onnx/pull/6443): Add LLM Normalization ops

This PR introduces various normalization ops used in LLMs, including RMS Normalization,
SkipLayerNormalization as well as SkipRMSNormalization. The latter two are simple
fused ops that combine an addition with the normalization step. The PR is ready
for review, and comments are welcome.

The RMS normalization op does not have training-related outputs (such as mean and
inverse standard deviation).

There was a discussion about unifying various normalization ops into one, a topic that
has come up previously. But there are minor differences, some support the use of a bias
and others don't, which could cause some complications in practice. In particular,
comments and suggestions welcome on whether RMS Normalization should support an
extra bias input for the sake of uniformity.

This op uses the same convention as other normalization ops in that the axis specified
indicates that all axes starting from that axis are normalized.

### [Issue 6408](https://github.com/onnx/onnx/pull/6408): Spec clarification for random ops

The ONNX ops for generating random numbers accept an optional seed attribute.
There is a question about the intended behavior in the two cases, when a seed is
specified and not specified. The behavior is different in onnxruntime and in the
onnx reference implementation.

When a seed is specified, should every execution of the op (node) produce the
same output (which is a natural interpretation from ONNX's general perspective
that ops are stateless) or should it act like a stateful op/node that uses the
seed only the first time it is executed (producing different values every time
it is executed)? The second interpretation does not fit well with ONNX's general
stateless op approach, but may be more useful in practice.

An alternative is that the seed can be made an input of the operator (instead of
it being a seed), and the operator can return an updated seed. The model must
then thread the seed value through the op invocations to achieve its desired
behavior. However, this could be complicated to implement in practice 
(in the exporters or in version-converters).

Another alternative is to recommend the use of random operators without any
seed attribute, and clarify in the specification that all random number
generation ops share a stateful seed. Setting this seed value (to produce
some deterministic behavior) can be done via APIs exposed by a runtime
(such as onnxruntime). The use of ops with a seed operator can be deprecated
in practice.

The consensus in the group was to follow the last alternative described
above.

